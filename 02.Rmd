# Data and Sampling Distributions

**Learning objectives:**

- Understand how to **sample from a population**.
- Identify various kinds of **sampling bias**.
- Know how to **avoid bias** in sampling.
- Understand the **distribution of a sample statistic**.
- Use the **bootstrap** to quantify **sampling variability**.
- Calculate **confidence intervals**.
- Recognize the most important common **distributions**.

## What is a Population?

- roughly, "a particular set of things we care about"
- may be concrete
  - the set of people who will vote in the next election
  - all the trees in some forest
- more generally, is *notional*
  - the space of outcomes from rolling a pair of dice
  - all possible offspring from a mating pair of fruit flies
  - the collection of physical *microstates* consistent with a given *macrostate*
  - etc.
 
## Populations {.unnumbered}

A *population* is represented as a *distribution* over one or more 
variables.

- voting populations are a distribution over the **candidates**
- tree populations are a distribution over the **species** of tree, the **diameter** of 
the trunk, the **number** of leaves, the **thickness** of the bark, etc.
- dice outcomes are a distribution over the **number rolled**
- etc.


## Population Statistics {.unnumbered}

The things we care about are *statistics* that can be calculated from the distribution.

- the mode of the candidate distribution
- the median height of the trees, divided by the MAD of the number of 
branchings^[I am sure that nobody cares about this metric. The point is that
a "statistic" can be *any* function of the distribution.]
- the mean and standard deviation of the number rolled
- etc.



## What is a Sample?

We almost never have full access to the population distribution that we care about,
so we have to settle for a *sample*.

 - consists of some number *n* of "individuals" from the population
   - poll 2000 likely voters
   - randomly select 50 trees from the forest to measure
   - roll the dice 100 times
 - drawn at **random** from the population
 - represented by a distribution over the same variables as the population
   
Whatever statistic we wanted to calculate for the population, we instead
   calculate for the sample.


## What is a Sample? {.unnumbered}


The book gives the url for a helpful demo:

![https://onlinestatbook.com/stat_sim/sampling_dist/](images/sampling_demo.png)


## We Have a Problem

The sample is not the population!

The sample statistics we calculate are not equal to the population statistics!

The sample statistic may differ from the population statistic for a variety of reasons:

- random fluctuation
- bias
  - selection bias (the sample may not have been drawn randomly from the population)
  - sample size bias^[the book glosses over this, so I don't know if there's a more
  standard term than just "bias"] (some sample metrics will be inherently and 
  systematically different from population metrics just because of the limited
  size of the sample)


## The Ideal Solution

*If* we had the resources to take many samples (e.g. 100 other research groups
doing the same study of the forest that we are), then we could do the following:

- Repeat the sampling process some number of times, taking a new random sample 
each time.
- For **each** sample, calculate the sample statistic.
- Make a histogram of all the resulting values for the sample statistic.

![](images/sampling_demo2.png)

## The Ideal Solution {.unnumbered}


The resulting **sampling distribution** of the statistic would help us understand
the results of our sampling experiment.

- The **mean** of the sampling distribution is (an estimate of) the value of the 
statistic that our experiment is "aiming at".^[This may **not** be equal to the 
population statistic, due to sample size bias. For example, if my target 
statistic is the range of some variable, the sample statistic will always be less
than or equal to the population statistic. And so the mean of the sampling 
distribution for the range will be less than the population range. 
Some statistics, like the mean, are known to be *unbiased statistics*, while 
others are known to be biased.]
- The **standard deviation** of the distribution is (an estimate of) how much 
random fluctuations are likely to influence our measurement. This is also known 
as the *standard error* of our calculated sample statistic.

![](images/sampling_demo3.png)

## The Central Limit Theorem 


In the special case where our sample statistic is the **mean**, it can be shown
that:

 - the sampling distribution approaches a **normal distribution**. 
 - an estimate of the standard deviation of that distribution is
given by the **standard deviation of an individual sample, divided by the square 
root of the sample size**.

![](images/sampling_demo4.png)


## The Bootstrap Solution

If our sample statistic is the mean^[or other statistic for which a theoretical 
approximation to the standard error has been derived], we can estimate the
standard error from a single sample. 

Otherwise, to measure the standard error of our sample statistic, we would have
to **repeat our sampling process many times** to be able to calculate the standard
deviation of the sampling distribution. (Of course, if we actually did this, our
calculated standard error would be pretty useless, because we'd have a better 
estimate of the statistic from the combined samples.)

In practice, we can use a **bootstrap**.

## The Bootstrap {.unnumbered}

The bootstrap is a simple but powerful technique for estimating the standard
error of *any* sample statistic from a single sample. We will also
obtain an estimate of the sample size bias for our sample statistic.

Sounds too good to be true. Where does this amazing capability come from?

## The Bootstrap {.unnumbered}

The bootstrap works like this:

- Assume that the sample distribution is representative of the population 
distribution.^[It doesn't have to be perfect, but if it's *too* far off then 
nothing you calculate from the sample will be meaningful anyway.]
- Construct a new *simulated* population by making a gazillion copies of your
sample (in practice, this just means you draw from the sample *with replacement*).
- Now it is easy to do lots of simulated experiments on your simulated population!
  - Draw lots of bootstrap samples from your simulated population, with each 
  sample having the same *n* as your original sample.
  - Calculate your sample statistic for each of the bootstrap samples.
  - Calculate the standard deviation of the resulting distribution (of 
  bootstrapped sample statistics); this is the estimated **standard error** for your
  measured sample statistic.
  - Calculate the difference between the mean of the bootstrap distribution and
  the statistic calculated on your original sample; this is the estimated **bias** for
  your measurement.


We can use the same sampling [demo](https://onlinestatbook.com/stat_sim/sampling_dist/)
to get a feel for how bootstrapping works.

## The Bootstrap: Limitations {.unnumbered}

The bootstrap isn't magic. 

It won't give you a better estimate of your sample statistic.^[Well, having an 
estimate of the sample size bias can help.]

It won't fix selection bias.

It won't fill gaps in your sample data.

It *does* help you understand the limitations of your experimental procedure.


## Confidence Intervals

A **confidence interval**, like the standard error, is a way to estimate the 
reliability of a sample statistic.

For example, a 95% CI is an interval that would contain the central 95% of values
for the sample statistic, if the sampling experiment were done a very large number
of times.

It's generally not practical to *actually* sample that many times, so...
bootstrap!

For comparison, plus or minus one SE gives a CI of about 68%. 

## Some Important Distributions

The rest of the chapter is about specific distributions.

Many of these distributions will come up again in particular contexts in 
following chapters.

## The Normal Distribution

```{r echo=FALSE, results=FALSE, message=FALSE}
library(dplyr)
library(ggplot2)
library(purrr)
```


```{r}
normal_values <- rnorm(n = 10000)

hist(normal_values, breaks = 30)

# qqnorm plots the location of quantiles of the given distribution
# vs locations of corresponding quantiles of normal distribution.

qqnorm(normal_values); qqline(normal_values, col = 2)
```

## Student's t-Distribution

```{r}
# t-distribution is a family parameterized by degrees of freedom
t_values <- rt(n = 10000, df = 10)

hist(t_values, breaks = 30)

# normalize distribution for QQ
t_values <- (t_values - mean(t_values))/sd(t_values)
qqnorm(t_values); qqline(t_values, col = 2)
```

## The Binomial Distribution

```{r}
# flipping 6 fair coins at a time, how many heads do we get?
binom_values <- rbinom(n = 10000, 6, 0.5)

hist(binom_values, breaks = 30)

# normalize distribution for QQ
binom_values <- (binom_values - mean(binom_values))/sd(binom_values)
qqnorm(binom_values); qqline(binom_values, col = 2)
```

## The Chi-Square Distribution

```{r}
# family of distributions parameterized by degrees of freedom
chisq_values <- rchisq(n = 10000, df = 5)

hist(chisq_values, breaks = 30)

# normalize distribution for QQ
chisq_values <- (chisq_values - mean(chisq_values))/sd(chisq_values)
qqnorm(chisq_values); qqline(chisq_values, col = 2)
```


## The F-Distribution

```{r}
# family of distributions parameterized by TWO df1 values
f_values <- rf(n = 10000, df1 = 15, df2 = 50)

hist(f_values, breaks = 30)


# normalize distribution for QQ
f_values <- (f_values - mean(f_values))/sd(f_values)
qqnorm(f_values); qqline(f_values, col = 2)

```



## The Poisson Distribution

```{r}
# family of distributions parameterized by lambda ("mean rate")
poisson_values <- rpois(n = 10000, lambda = 5)

hist(poisson_values, breaks = 30)

# normalize distribution for QQ
poisson_values <- (poisson_values - mean(poisson_values))/sd(poisson_values)
qqnorm(poisson_values); qqline(poisson_values, col = 2)
```


## Meeting Videos

### Cohort 1

`r knitr::include_url("https://www.youtube.com/embed/w_UZIKzstnU")`

<details>
<summary> Meeting chat log </summary>

```
00:09:49	shamsuddeen:	Hi everyone, good to see u all today
00:10:36	Jon Harmon (jonthegeek):	Good morning (or whatever time it might be)!
00:12:51	priyanka gagneja:	I see chap 2
00:14:32	Jon Harmon (jonthegeek):	Did someone volunteer for Chapter 3? I'll bother people at the end if not but thought I'd let people mull it over before then :)
00:16:48	Morgan Grovenburg:	Are we meeting on Memorial Day?
00:17:31	Jon Harmon (jonthegeek):	Oh, hmm. We should skip. Thanks for noticing the timing! That also gives us more time to get caught up for chapter 3 :)
00:22:40	Scott Nestler:	Minor quibble on something that was said. But I think it is important. I would say that a sample HAS a distribution (the sampling distribution), rather than IS a distribution.  Any particular sample we draw is one observation FROM the sampling distribution.
00:23:12	Scott Nestler:	(technically a collection of individual observations)
00:23:25	Diego Ram√≠rez Gonz√°lez:	When you do hypothesis testing you want to estimate something about a population, but you do it through a sample
00:24:20	Scott Nestler:	If you have access to the population, and you measure things about each member, you are conducting a CENSUS.
00:24:52	Kaytee Flick:	@Scott The book defines the sampling distribution as the distribution of some sample stat over many samples from the same population....vs data distribution which is the distribution of individual data points
00:24:58	Diego Ram√≠rez Gonz√°lez:	Isn't a sampling distribution a distribution a point estimates?
00:25:06	shamsuddeen:	@Scot great point
00:25:14	Diego Ram√≠rez Gonz√°lez:	while a sample has a distribution of raw data
00:26:44	priyanka gagneja:	@diego .. I would say that's an ideal scenario .. if your sample is BEST ( I think that's the acronym or something) .. where your sample is representative of the population.. it would have same distribution as population but that may not always be true 
00:28:04	shamsuddeen:	Can we have bias-free data?
00:28:48	priyanka gagneja:	oh and a suggestion/ correction Jonathan.. from my old stats class .. population has (p)arameters while sample has (s)tatistic
00:32:59	Scott Nestler:	Priyanka is correct that populations have parameters (that describe the distribution for named families), BUT they do also have "population statistics" which can sometimes (but rarely) be calculated.
00:36:14	Scott Nestler:	Regarding Shamsuddeen's question ‚Ä¶ it really isn't data that has a bias (or not), but rather the statistic(s) we are calculating with it.  Some statistics are biased and some are not. That depends on whether the expected value of the statistic is equal to the population parameter being estimated by it (or not).
00:36:54	priyanka gagneja:	+1 ..Scott 's ans to shamshudeen s ques
00:38:53	shamsuddeen:	Thanks Scott.
00:39:56	pavitra:	amazing discussion Jonathan. Really eye-opening when you get into the weeds with summary stats
00:42:03	pavitra:	even if sample size is small, sampling enough number of times eventually gets you a normally distributed summary stat?
00:42:41	jiwan:	I think the book covers this in the bootstrap section
00:43:13	Jon Harmon (jonthegeek):	He's building up to the bootstrap üòÑ
00:45:15	Diego Ram√≠rez Gonz√°lez:	Showing the standard error is a bit misleading anyway, its always going to be smaller than the standard deviation
00:48:01	Kaytee Flick:	.....I think that's exactly why its used.
00:52:31	Rahul:	1.272
00:59:22	Jon Harmon (jonthegeek):	If you have 1e: The Chi-Square Distribution and F-Distribution subsections are new... but they talk about those distributions in later chapters so you should still be fine/able to keep up when we talk about them again.
01:00:21	pavitra:	dang, this chapter is everything
01:03:13	Scott Nestler:	Regarding skill vs. luck in sports (briefly mentioned in the chapter) -- the short video here is useful:  https://stakehunters.com/betting-guide/the-balance-of-luck-and-skills-in-top-sports--choose-on-what-do-you-bet
01:04:00	jiwan:	ch,3 was fairly long
01:04:32	Morgan Grovenburg:	I'd like to split the chapters in half
01:04:54	Morgan Grovenburg:	I can't present that week
01:05:29	Diego Ram√≠rez Gonz√°lez:	maybe we should combine chapter 3 and 4, they are basically the same information
01:07:26	Diego Ram√≠rez Gonz√°lez:	bye
```
</details>
