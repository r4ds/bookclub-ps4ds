[["index.html", "Practical Statistics for Data Scientists Book Club Welcome", " Practical Statistics for Data Scientists Book Club The R4DS Online Learning Community 2021-08-02 Welcome This is a companion for the book Practical Statistics for Data Scientists by Peter Bruce, Andrew Bruce, and Peter Gedeck (O’Reilly, copyright 2020, 978-1-492-07294-2). This companion is available at r4ds.io/ps4ds. This website is being developed by the R4DS Online Learning Community. Follow along, and join the community to participate. This companion follows the R4DS Online Learning Community Code of Conduct. "],["book-club-meetings.html", "Book club meetings", " Book club meetings Each week, a volunteer will present a chapter from the book (or part of a chapter). This is the best way to learn the material. Presentations will usually consist of a review of the material, a discussion, and/or a demonstration of the principles presented in that chapter. More information about how to present is available in the github repo. Presentations will be recorded, and will be available on the R4DS Online Learning Community YouTube Channel. "],["sample-code.html", "Sample code", " Sample code Sample code is available at github.com/gedeck/practical-statistics-for-data-scientists You can install all packages used by these notes and recommended by the book authors1: install.packages(&quot;remotes&quot;) remotes::install_github(&quot;r4ds/bookclub-ps4ds&quot;) remove.packages(&quot;ps4ds&quot;) # This isn&#39;t really a package. Chapter 5 uses {DMwR}, which is currently unavailable on CRAN. We’ll confirm when we get there, but I’m guessing {DMwR2} will work instead.↩︎ "],["st-edition-vs-2nd-edition.html", "1st edition vs 2nd edition", " 1st edition vs 2nd edition Here we’ll attempt to document differences between 1e and 2e. 2e added Python example code Some sections and subsections have slight name changes Chapter 1: New subsection in 1.6, “Probability” Chapter 2: New sections “Chi-Square Distribution” &amp; “F-Distribution” Chapter 4: New subsection in 4.2, “Further Reading” Chapter 7: New subsection in 7.1, “Correspondence Analysis” "],["pace.html", "Pace", " Pace Chapters are long, but not always dense. We’ll try to cover 1 chapter/week, but… …It’s ok to split chapters when they feel like too much. "],["meeting-videos.html", "0.1 Meeting Videos", " 0.1 Meeting Videos 0.1.1 Cohort 1 Meeting chat log 00:12:31 Jon Harmon (jonthegeek): r4ds.io/ps4ds 00:14:47 Bryan Tegomoh: 2nd edition 00:14:49 Scott Nestler: I have a copy of each. 00:14:50 Francisco Escobar: 1e 00:17:26 Scott Nestler: Sounds like a good plan to start with. 00:18:28 Kaytee Flick: I liked that part:D 00:21:29 Jon Harmon (jonthegeek): For those synching up later: &quot;I liked that part&quot; was about the Tukey stuff in chapter 1. 00:32:34 Diego Ramírez González: I am a neuroscience grad student in Mexico. I am here to study stats :) 00:32:58 Kaytee Flick: Yay neuroscience! 00:34:49 Morgan Grovenburg: https://github.com/r4ds/bookclub-ps4ds 00:38:03 Kaytee Flick: Can confirm that learning both simultaneously can break your brain....speaking from my current experience:P 00:38:22 shamsuddeen: @Kaytee..-;) "],["exploratory-data-analysis.html", "Chapter 1 Exploratory Data Analysis", " Chapter 1 Exploratory Data Analysis Learning objectives: Classify data as numeric or categorical. Compare and contrast estimates of location. Compare and contrast estimates of variability. Visualize data distributions. Visualize categorical data. Use correlation coefficients to measure association between two variables. Visualize data distributions in two dimensions. "],["structured-data.html", "1.1 Structured Data", " 1.1 Structured Data Software classifies data by type. Numeric (continuous or discrete) Categorical (binary, ordinal, neither) Rectangular data = typical frame of reference for data science. Called a data.frame in R Rows are records (aka observations, cases, instances) Columns are features (aka variables, attributes, predictors in some cases) Lots of synonyms in stats and data science for same things. "],["estimates-of-location.html", "1.2 Estimates of Location", " 1.2 Estimates of Location Most basic = mean. dataset &lt;- c(3, 4, 1, 2, 10) mean(dataset) # (3 + 4 + 1 + 2 + 10)/5 = 20/5 ## [1] 4 Trimming helps eliminate outliers mean(dataset, trim = 1/5) # (2 + 3 + 4)/3 = 9/3 ## [1] 3 Weight to: Down-weight high-variability values. Up-weight under-represented values. weights &lt;- c(1, 1, 11, 1, 1) weighted.mean(dataset, weights) # (3 + 4 + 11 + 2 + 10)/15 = 30/15 ## [1] 2 Median: sort then choose middle value. median(dataset) # 1, 2, (3), 4, 10 ## [1] 3 Weighted median: similar to weighted mean, but more complicated. # Sort then weight then middle of weight. 1*11, 2*1, 3*1, 4*1, 10*1 matrixStats::weightedMedian(dataset, weights) ## [1] 1.333333 Technically it interpolates in-between values. matrixStats::weightedMedian(dataset, weights, interpolate = TRUE) ## [1] 1.333333 Can tell it not to interpolate to simplify. matrixStats::weightedMedian(dataset, weights, interpolate = FALSE) ## [1] 1 # Equivalent to repeating values weight times. median(c(rep(1, 11), 2, 3, 4, 10)) ## [1] 1 Their sample code is available at github.com/gedeck/practical-statistics-for-data-scientists "],["estimates-of-variability.html", "1.3 Estimates of Variability", " 1.3 Estimates of Variability Variability (aka dispersion) = are values clustered or spread out? 1.3.1 SD &amp; Friends Variance = average of squared deviations, \\(s^2 = \\frac{\\sum_{i=1}^{n}{(x_{1}-\\bar{x})^2}}{n-1}\\) s_squared &lt;- var(dataset) s_squared ## [1] 12.5 Standard deviation = square root of variance, \\(s = \\sqrt{variance}\\) s &lt;- sd(dataset) s ## [1] 3.535534 s == sqrt(s_squared) ## [1] TRUE Median absolute deviation from the median (MAD) is robust to outliers. mad(dataset) ## [1] 1.4826 Wait, why did that return the standard scale factor? dataset is c(1, 2, 3, 4, 10) The difference between any 2 values is 1 (except the outlier) 1 * 1.4826 = 1.4826 1.3.2 Percentiles &amp; Friends Percentiles = quantiles, \\(P\\%\\) of values are \\(&lt;= x\\) x &lt;- sample(1:100, 100, replace = TRUE) y &lt;- rnorm(100, mean = 50, sd = 20) quantile(x, probs = seq(0, 1, 0.1)) ## 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% ## 6.0 17.0 22.0 29.0 36.8 46.0 62.4 70.0 80.2 86.2 100.0 quantile(y, probs = seq(0, 1, 0.1)) ## 0% 10% 20% 30% 40% 50% 60% 70% ## -7.844636 23.122958 30.553372 39.973669 44.028722 49.467467 52.334689 59.097999 ## 80% 90% 100% ## 66.340103 75.125117 99.387869 quantile(x) # quartile ## 0% 25% 50% 75% 100% ## 6.00 26.00 46.00 75.25 100.00 IQR(x) # They introduce this later but I like it here. ## [1] 49.25 "],["histograms-friends.html", "1.4 Histograms &amp; Friends", " 1.4 Histograms &amp; Friends state &lt;- read.csv(&quot;data/state.csv&quot;) head(state) ## State Population Murder.Rate Abbreviation ## 1 Alabama 4779736 5.7 AL ## 2 Alaska 710231 5.6 AK ## 3 Arizona 6392017 4.7 AZ ## 4 Arkansas 2915918 5.6 AR ## 5 California 37253956 4.4 CA ## 6 Colorado 5029196 2.8 CO library(ggplot2) ggplot(state, aes(y = Population/1000000)) + geom_boxplot() + ylab(&quot;Population (millions)&quot;) ggplot(state, aes(x = Population/1000000)) + geom_histogram( aes(y = after_stat(density)), bins = 10, fill = &quot;white&quot;, color = &quot;black&quot; ) + geom_density(fill = &quot;blue&quot;, alpha = 0.5) + xlab(&quot;Population (millions)&quot;) "],["visualizing-categorical-data.html", "1.5 Visualizing Categorical Data", " 1.5 Visualizing Categorical Data Bar charts are boring. We’ll see some examples related to this in 2D. "],["correlation.html", "1.6 Correlation", " 1.6 Correlation library(corrplot) ## corrplot 0.90 loaded library(dplyr, quietly = TRUE) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union sp500_px &lt;- read.csv(&quot;data/sp500_data.csv.gz&quot;) %&gt;% as_tibble() sp500_sym &lt;- read.csv(&quot;data/sp500_sectors.csv&quot;, stringsAsFactors = FALSE) %&gt;% as_tibble() etfs &lt;- sp500_px %&gt;% filter(X &gt; &quot;2012-07-01&quot;) %&gt;% select( any_of( sp500_sym %&gt;% filter(sector == &quot;etf&quot;) %&gt;% pull(symbol) ) ) corrplot(cor(etfs), method = &quot;ellipse&quot;) "],["d-distributions.html", "1.7 2D Distributions", " 1.7 2D Distributions https://xkcd.com/1967/ "],["meeting-videos-1.html", "1.8 Meeting Videos", " 1.8 Meeting Videos 1.8.1 Cohort 1 Meeting chat log 00:04:49 Madeline Arnold (she/her): Hello everyone! I’m eating breakfast so going to have my camera off for now :) 00:15:08 Scott Nestler: Effectively, the median is a trimmed mean that removes 50% of the lower and upper values in a data set. 00:15:33 Morgan Grovenburg: Haha! 00:16:38 Scott Nestler: Also, if you need a standard error (SE) calculation, you can&#39;t do that with a median, but you can with a trimmed mean, for both normal and non-normal data. 00:17:56 Scott Nestler: Actually, I mis-spoke. You can calculate the SE of the median, but it is generally higher than for the mean (or trimmed mean). So trimmed mean is a balance of resistance to outliers and providing a low SE. 00:20:14 pavitra: Scott, so is trimmed mean best practice, or median? 00:21:33 Jone Aliri: the trimmed mean it&#39;s less conservative than the median 00:21:53 Jone Aliri: what is best practice depends on the data 00:22:10 Diego Ramírez González: A trimmed mean is probably not a good idea if you don&#39;t have a lot of data points 00:22:46 pavitra: makes sense! 00:24:24 Madeline Arnold (she/her): For folks who’ve used trimmed mean (new to me!) do you often use the 10% percentile cutoff described in PS4DS or some other cutoff for outliers? 00:27:04 Scott Nestler: https://en.wikipedia.org/wiki/Median_absolute_deviation#Relation_to_standard_deviation 00:29:10 pavitra: there are no dumb questions 00:33:37 Madeline Arnold (she/her): In my experience in biology research, if spread of data is bigger it means I need a bigger n (need to have more samples to be confident about the estimated mean being accurate) 00:33:53 Kaytee Flick: Same Madeline...that&#39;s what I was thinking about 00:35:18 shamsuddeen: Thanks 00:38:36 Diego Ramírez González: if you increase the sample size the mean and the median (50th percentile) will be closer 00:39:42 jonathan.bratt: And maybe deciles rather than percentiles are easier to read for this example. 00:45:48 Morgan Grovenburg: &lt;3 boxplots 00:45:50 Scott Nestler: I really like comparative box plots, when you are trying to look at the distribution of data for 2 or more categories. 00:45:56 pavitra: still cant see distribution in boxplots 00:46:51 Diego Ramírez González: the box is the 25th and 75th percentiles, the line in the middle of the box is the median, the whiskers are 1.5*IQR and the points outside are outliers 00:47:10 Rahul: This was very helpful to understand why 1.5 is used https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51 00:47:18 Francisco Escobar: 1.58 gives ~95% confidence interval for the median https://ggplot2.tidyverse.org/reference/geom_boxplot.html 00:47:38 Diego Ramírez González: i like boxplot, but violin plots are better :) 00:47:43 Diego Ramírez González: boxplots* 00:48:02 Anne Hoffrichter: I like the combination of violin and boxplots ;) 00:48:04 Kaytee Flick: I was just going to ask how we feel about violin plots:P 00:48:17 jiwan: https://twitter.com/CedScherer/status/1387155336998670344/photo/1 00:48:19 Diego Ramírez González: yeah, violin + boxplot is even better 00:49:41 pavitra: the good ole&#39; area under curve 00:49:54 Diego Ramírez González: this is my favorite gif :) https://twitter.com/danilobzdok/status/1341893126592593924 00:50:46 Kaytee Flick: That&#39;s epic 00:50:50 Scott Nestler: Ridgeline plots! 00:50:51 Madeline Arnold (she/her): @Diego love it 00:51:06 pavitra: is there something called &quot;raindrop&quot; plot or something? 00:51:54 pavitra: I really like raindrop plots 00:51:58 Diego Ramírez González: anything but dynamite plots 00:53:31 Diego Ramírez González: i agree, those two distributions don&#39;t even overlap, the dynamite plot is not so bad here 00:53:33 pavitra: Sorry to belabor this. However, why is MAD not as prevalent as SD? No rush to answer here - maybe we can discuss further in the slack channel 00:54:59 Jone Aliri: It&#39;s because MAD is calculated with absolute values 00:55:06 pavitra: Nassim Taleb loves MAD and says it is more accurate for skewed data 00:55:56 pavitra: Good point, Jone 00:56:04 Scott Nestler: Also, the absolute value function is non-smooth, which used to create all sorts of calculation issues. 01:01:37 pavitra: great presentation, Jon! 01:02:41 priyanka gagneja: Bi-modal you mean 01:03:12 pavitra: thanks a lot, y&#39;all.Gotta go. 01:03:35 Jone Aliri: And the other problema is that teh distance could be different from 1 to 2 or from 3 to 4 in Likert 01:04:55 Diego Ramírez González: i guess it depends on the distribution of your data and what assumptions you are willing to make about the measurement 01:05:59 Madeline Arnold (she/her): I hope we learn more about this topic! Thanks for the question Sheila 01:06:22 Kaytee Flick: Congrats Scott!!! 01:06:35 Jone Aliri: In psychology we use a lot of scales with Likert... and a lot of times we add them... 01:07:23 Scott Nestler: A good way to remember not to use means on Likert scale data is to think: The average of Agree and Strongly Agree is not Agree-And-A-Half. 01:07:30 Scott Nestler: https://bookdown.org/Rmadillo/likert/summary.html 01:07:53 Diego Ramírez González: yeah, but for some of these instruments people will add the answers to get a total score 01:08:13 Jim Gruman: not sure if this adds anything, but there are domains where a geometric mean is more appropriate than the average 01:08:29 Morgan Grovenburg: I don&#39;t have a good answer, but I use the `HH` package to visualized likert scales https://xang1234.github.io/likert/ 01:09:06 Jim Gruman: and in market research, my company skips to valuing only the &quot;top box scores&quot; 01:10:01 Jone Aliri: Yes @Diego that&#39;s it, we get the total score which we can use like a continuos scale :) 01:10:11 Jim Gruman: thank you!! 01:10:16 Andrew G. Farina: Thanks Jon! "],["data-and-sampling-distributions.html", "Chapter 2 Data and Sampling Distributions", " Chapter 2 Data and Sampling Distributions Learning objectives: Understand how to sample from a population. Identify various kinds of sampling bias. Know how to avoid bias in sampling. Understand the distribution of a sample statistic. Use the bootstrap to quantify sampling variability. Calculate confidence intervals. Recognize the most important common distributions. "],["what-is-a-population.html", "2.1 What is a Population?", " 2.1 What is a Population? roughly, “a particular set of things we care about” may be concrete the set of people who will vote in the next election all the trees in some forest more generally, is notional the space of outcomes from rolling a pair of dice all possible offspring from a mating pair of fruit flies the collection of physical microstates consistent with a given macrostate etc. "],["populations.html", "Populations", " Populations A population is represented as a distribution over one or more variables. voting populations are a distribution over the candidates tree populations are a distribution over the species of tree, the diameter of the trunk, the number of leaves, the thickness of the bark, etc. dice outcomes are a distribution over the number rolled etc. "],["population-statistics.html", "Population Statistics", " Population Statistics The things we care about are statistics that can be calculated from the distribution. the mode of the candidate distribution the median height of the trees, divided by the MAD of the number of branchings2 the mean and standard deviation of the number rolled etc. I am sure that nobody cares about this metric. The point is that a “statistic” can be any function of the distribution.↩︎ "],["what-is-a-sample.html", "2.2 What is a Sample?", " 2.2 What is a Sample? We almost never have full access to the population distribution that we care about, so we have to settle for a sample. consists of some number n of “individuals” from the population poll 2000 likely voters randomly select 50 trees from the forest to measure roll the dice 100 times drawn at random from the population represented by a distribution over the same variables as the population Whatever statistic we wanted to calculate for the population, we instead calculate for the sample. "],["what-is-a-sample-1.html", "What is a Sample?", " What is a Sample? The book gives the url for a helpful demo: https://onlinestatbook.com/stat_sim/sampling_dist/ "],["we-have-a-problem.html", "2.3 We Have a Problem", " 2.3 We Have a Problem The sample is not the population! The sample statistics we calculate are not equal to the population statistics! The sample statistic may differ from the population statistic for a variety of reasons: random fluctuation bias selection bias (the sample may not have been drawn randomly from the population) sample size bias3 (some sample metrics will be inherently and systematically different from population metrics just because of the limited size of the sample) the book glosses over this, so I don’t know if there’s a more standard term than just “bias”↩︎ "],["the-ideal-solution.html", "2.4 The Ideal Solution", " 2.4 The Ideal Solution If we had the resources to take many samples (e.g. 100 other research groups doing the same study of the forest that we are), then we could do the following: Repeat the sampling process some number of times, taking a new random sample each time. For each sample, calculate the sample statistic. Make a histogram of all the resulting values for the sample statistic. "],["the-ideal-solution-1.html", "The Ideal Solution", " The Ideal Solution The resulting sampling distribution of the statistic would help us understand the results of our sampling experiment. The mean of the sampling distribution is (an estimate of) the value of the statistic that our experiment is “aiming at.”4 The standard deviation of the distribution is (an estimate of) how much random fluctuations are likely to influence our measurement. This is also known as the standard error of our calculated sample statistic. This may not be equal to the population statistic, due to sample size bias. For example, if my target statistic is the range of some variable, the sample statistic will always be less than or equal to the population statistic. And so the mean of the sampling distribution for the range will be less than the population range. Some statistics, like the mean, are known to be unbiased statistics, while others are known to be biased.↩︎ "],["the-central-limit-theorem.html", "2.5 The Central Limit Theorem", " 2.5 The Central Limit Theorem In the special case where our sample statistic is the mean, it can be shown that: the sampling distribution approaches a normal distribution. an estimate of the standard deviation of that distribution is given by the standard deviation of an individual sample, divided by the square root of the sample size. "],["the-bootstrap-solution.html", "2.6 The Bootstrap Solution", " 2.6 The Bootstrap Solution If our sample statistic is the mean5, we can estimate the standard error from a single sample. Otherwise, to measure the standard error of our sample statistic, we would have to repeat our sampling process many times to be able to calculate the standard deviation of the sampling distribution. (Of course, if we actually did this, our calculated standard error would be pretty useless, because we’d have a better estimate of the statistic from the combined samples.) In practice, we can use a bootstrap. or other statistic for which a theoretical approximation to the standard error has been derived↩︎ "],["the-bootstrap.html", "The Bootstrap", " The Bootstrap The bootstrap is a simple but powerful technique for estimating the standard error of any sample statistic from a single sample. We will also obtain an estimate of the sample size bias for our sample statistic. Sounds too good to be true. Where does this amazing capability come from? "],["the-bootstrap-1.html", "The Bootstrap", " The Bootstrap The bootstrap works like this: Assume that the sample distribution is representative of the population distribution.6 Construct a new simulated population by making a gazillion copies of your sample (in practice, this just means you draw from the sample with replacement). Now it is easy to do lots of simulated experiments on your simulated population! Draw lots of bootstrap samples from your simulated population, with each sample having the same n as your original sample. Calculate your sample statistic for each of the bootstrap samples. Calculate the standard deviation of the resulting distribution (of bootstrapped sample statistics); this is the estimated standard error for your measured sample statistic. Calculate the difference between the mean of the bootstrap distribution and the statistic calculated on your original sample; this is the estimated bias for your measurement. We can use the same sampling demo to get a feel for how bootstrapping works. It doesn’t have to be perfect, but if it’s too far off then nothing you calculate from the sample will be meaningful anyway.↩︎ "],["the-bootstrap-limitations.html", "The Bootstrap: Limitations", " The Bootstrap: Limitations The bootstrap isn’t magic. It won’t give you a better estimate of your sample statistic.7 It won’t fix selection bias. It won’t fill gaps in your sample data. It does help you understand the limitations of your experimental procedure. Well, having an estimate of the sample size bias can help.↩︎ "],["confidence-intervals.html", "2.7 Confidence Intervals", " 2.7 Confidence Intervals A confidence interval, like the standard error, is a way to estimate the reliability of a sample statistic. For example, a 95% CI is an interval that would contain the central 95% of values for the sample statistic, if the sampling experiment were done a very large number of times. It’s generally not practical to actually sample that many times, so… bootstrap! For comparison, plus or minus one SE gives a CI of about 68%. "],["some-important-distributions.html", "2.8 Some Important Distributions", " 2.8 Some Important Distributions The rest of the chapter is about specific distributions. Many of these distributions will come up again in particular contexts in following chapters. "],["the-normal-distribution.html", "2.9 The Normal Distribution", " 2.9 The Normal Distribution normal_values &lt;- rnorm(n = 10000) hist(normal_values, breaks = 30) # qqnorm plots the location of quantiles of the given distribution # vs locations of corresponding quantiles of normal distribution. qqnorm(normal_values); qqline(normal_values, col = 2) "],["students-t-distribution.html", "2.10 Student’s t-Distribution", " 2.10 Student’s t-Distribution # t-distribution is a family parameterized by degrees of freedom t_values &lt;- rt(n = 10000, df = 10) hist(t_values, breaks = 30) # normalize distribution for QQ t_values &lt;- (t_values - mean(t_values))/sd(t_values) qqnorm(t_values); qqline(t_values, col = 2) "],["the-binomial-distribution.html", "2.11 The Binomial Distribution", " 2.11 The Binomial Distribution # flipping 6 fair coins at a time, how many heads do we get? binom_values &lt;- rbinom(n = 10000, 6, 0.5) hist(binom_values, breaks = 30) # normalize distribution for QQ binom_values &lt;- (binom_values - mean(binom_values))/sd(binom_values) qqnorm(binom_values); qqline(binom_values, col = 2) "],["the-chi-square-distribution.html", "2.12 The Chi-Square Distribution", " 2.12 The Chi-Square Distribution # family of distributions parameterized by degrees of freedom chisq_values &lt;- rchisq(n = 10000, df = 5) hist(chisq_values, breaks = 30) # normalize distribution for QQ chisq_values &lt;- (chisq_values - mean(chisq_values))/sd(chisq_values) qqnorm(chisq_values); qqline(chisq_values, col = 2) "],["the-f-distribution.html", "2.13 The F-Distribution", " 2.13 The F-Distribution # family of distributions parameterized by TWO df1 values f_values &lt;- rf(n = 10000, df1 = 15, df2 = 50) hist(f_values, breaks = 30) # normalize distribution for QQ f_values &lt;- (f_values - mean(f_values))/sd(f_values) qqnorm(f_values); qqline(f_values, col = 2) "],["the-poisson-distribution.html", "2.14 The Poisson Distribution", " 2.14 The Poisson Distribution # family of distributions parameterized by lambda (&quot;mean rate&quot;) poisson_values &lt;- rpois(n = 10000, lambda = 5) hist(poisson_values, breaks = 30) # normalize distribution for QQ poisson_values &lt;- (poisson_values - mean(poisson_values))/sd(poisson_values) qqnorm(poisson_values); qqline(poisson_values, col = 2) "],["meeting-videos-2.html", "2.15 Meeting Videos", " 2.15 Meeting Videos 2.15.1 Cohort 1 Meeting chat log 00:09:49 shamsuddeen: Hi everyone, good to see u all today 00:10:36 Jon Harmon (jonthegeek): Good morning (or whatever time it might be)! 00:12:51 priyanka gagneja: I see chap 2 00:14:32 Jon Harmon (jonthegeek): Did someone volunteer for Chapter 3? I&#39;ll bother people at the end if not but thought I&#39;d let people mull it over before then :) 00:16:48 Morgan Grovenburg: Are we meeting on Memorial Day? 00:17:31 Jon Harmon (jonthegeek): Oh, hmm. We should skip. Thanks for noticing the timing! That also gives us more time to get caught up for chapter 3 :) 00:22:40 Scott Nestler: Minor quibble on something that was said. But I think it is important. I would say that a sample HAS a distribution (the sampling distribution), rather than IS a distribution. Any particular sample we draw is one observation FROM the sampling distribution. 00:23:12 Scott Nestler: (technically a collection of individual observations) 00:23:25 Diego Ramírez González: When you do hypothesis testing you want to estimate something about a population, but you do it through a sample 00:24:20 Scott Nestler: If you have access to the population, and you measure things about each member, you are conducting a CENSUS. 00:24:52 Kaytee Flick: @Scott The book defines the sampling distribution as the distribution of some sample stat over many samples from the same population....vs data distribution which is the distribution of individual data points 00:24:58 Diego Ramírez González: Isn&#39;t a sampling distribution a distribution a point estimates? 00:25:06 shamsuddeen: @Scot great point 00:25:14 Diego Ramírez González: while a sample has a distribution of raw data 00:26:44 priyanka gagneja: @diego .. I would say that&#39;s an ideal scenario .. if your sample is BEST ( I think that&#39;s the acronym or something) .. where your sample is representative of the population.. it would have same distribution as population but that may not always be true 00:28:04 shamsuddeen: Can we have bias-free data? 00:28:48 priyanka gagneja: oh and a suggestion/ correction Jonathan.. from my old stats class .. population has (p)arameters while sample has (s)tatistic 00:32:59 Scott Nestler: Priyanka is correct that populations have parameters (that describe the distribution for named families), BUT they do also have &quot;population statistics&quot; which can sometimes (but rarely) be calculated. 00:36:14 Scott Nestler: Regarding Shamsuddeen&#39;s question … it really isn&#39;t data that has a bias (or not), but rather the statistic(s) we are calculating with it. Some statistics are biased and some are not. That depends on whether the expected value of the statistic is equal to the population parameter being estimated by it (or not). 00:36:54 priyanka gagneja: +1 ..Scott &#39;s ans to shamshudeen s ques 00:38:53 shamsuddeen: Thanks Scott. 00:39:56 pavitra: amazing discussion Jonathan. Really eye-opening when you get into the weeds with summary stats 00:42:03 pavitra: even if sample size is small, sampling enough number of times eventually gets you a normally distributed summary stat? 00:42:41 jiwan: I think the book covers this in the bootstrap section 00:43:13 Jon Harmon (jonthegeek): He&#39;s building up to the bootstrap 😄 00:45:15 Diego Ramírez González: Showing the standard error is a bit misleading anyway, its always going to be smaller than the standard deviation 00:48:01 Kaytee Flick: .....I think that&#39;s exactly why its used. 00:52:31 Rahul: 1.272 00:59:22 Jon Harmon (jonthegeek): If you have 1e: The Chi-Square Distribution and F-Distribution subsections are new... but they talk about those distributions in later chapters so you should still be fine/able to keep up when we talk about them again. 01:00:21 pavitra: dang, this chapter is everything 01:03:13 Scott Nestler: Regarding skill vs. luck in sports (briefly mentioned in the chapter) -- the short video here is useful: https://stakehunters.com/betting-guide/the-balance-of-luck-and-skills-in-top-sports--choose-on-what-do-you-bet 01:04:00 jiwan: ch,3 was fairly long 01:04:32 Morgan Grovenburg: I&#39;d like to split the chapters in half 01:04:54 Morgan Grovenburg: I can&#39;t present that week 01:05:29 Diego Ramírez González: maybe we should combine chapter 3 and 4, they are basically the same information 01:07:26 Diego Ramírez González: bye "],["statistical-experiments-and-significance-testing.html", "Chapter 3 Statistical Experiments and Significance Testing", " Chapter 3 Statistical Experiments and Significance Testing Main point: resampling is the best way to test the significance of a statistical experiment. Learning objectives: Design A/B tests. Use hypothesis tests to understand the results of a statistical experiment. Perform resampling procedures to test the significance of statistical experiments. Explain the proper uses (and abuses!) of p-values. Compare and contrast the various traditional tests of significance. Explain the advantages of multi-armed bandit tests over traditional A/B tests. Calculate the statistical power for a statistical experiment. "],["slide-2.html", "3.1 SLIDE 2", " 3.1 SLIDE 2 ADD SLIDE CONTENTS "],["slide-3.html", "3.2 SLIDE 3", " 3.2 SLIDE 3 MORE SLIDE CONTENTS "],["meeting-videos-3.html", "3.3 Meeting Videos", " 3.3 Meeting Videos 3.3.1 Cohort 1 Meeting chat log no chat log "],["regression-and-prediction.html", "Chapter 4 Regression and Prediction", " Chapter 4 Regression and Prediction Learning objectives: Perform linear regressions with a single independent variable. Perform linear regressions with multiple independent variables. Perform regressions with one or more categorical independent variables. Perform nonlinear generalizations of regression. Compare and contrast the use of regression for prediction vs. explanation. Cautiously interpret the results of a multivariable linear regression. Assess the goodness of a regression model. "],["weighted-regression.html", "4.1 Weighted Regression", " 4.1 Weighted Regression Used to give certain records (variables, features) more or less weighting when fitting the regression model. To show this, I will use the ames housing data from the {modeldata} package from tidymodels and prioritize sale prices of houses sold more recently than those sold earlier in these data. dat &lt;- ames %&gt;% dplyr::select(Lot_Area, Neighborhood, Year_Sold, First_Flr_SF, Second_Flr_SF, Bsmt_Full_Bath, Full_Bath, Half_Bath, Bsmt_Half_Bath, Sale_Price, Bedroom_AbvGr, Central_Air, Bldg_Type) %&gt;% dplyr::mutate(weight = Year_Sold - 2006, total_sf = First_Flr_SF + Second_Flr_SF, bath = Bsmt_Full_Bath + Full_Bath + 0.5*Half_Bath + 0.5*Bsmt_Half_Bath) house_lm &lt;- lm(Sale_Price ~ total_sf + Lot_Area + bath + Bedroom_AbvGr + Central_Air, data = dat) house_wt &lt;- lm(Sale_Price ~ total_sf + Lot_Area + bath + Bedroom_AbvGr + Central_Air, data = dat, weight = weight) round(cbind(house_lm = house_lm$coefficients, house_wt = house_wt$coefficients), digits = 3) ## house_lm house_wt ## (Intercept) -4804.696 -3943.203 ## total_sf 104.742 104.705 ## Lot_Area 0.605 0.673 ## bath 25411.278 24754.042 ## Bedroom_AbvGr -25438.440 -27218.685 ## Central_AirY 41924.976 45938.683 "],["prediction-using-regression.html", "4.2 Prediction using Regression", " 4.2 Prediction using Regression Caution: Be careful extrapolating results beyond the range of the dataset Prediction Interval (Uncertainty around a single value) Confidence Intervals (Uncertainty around a statistic) Individual data point error Here is an example of individual data point error. If we filter for those properties that have four bedrooms, 3 bathrooms and a lot square footage between 10k and 11k, the sale price varies by $50k. This is error in our model. dat %&gt;% dplyr::filter(Bedroom_AbvGr == 4 &amp; bath == 3 &amp; Lot_Area &gt;=10000 &amp; Lot_Area &lt; 11000) %&gt;% dplyr::select(Sale_Price) %&gt;% dplyr::arrange(-Sale_Price) ## # A tibble: 9 × 1 ## Sale_Price ## &lt;int&gt; ## 1 218500 ## 2 211000 ## 3 170000 ## 4 165150 ## 5 157000 ## 6 139000 ## 7 127500 ## 8 103500 ## 9 100000 "],["factor-variables.html", "4.3 Factor Variables", " 4.3 Factor Variables We can use the building type as a factor variable to help with our predictions. The building type variable has five options: dat %&gt;% count(Bldg_Type) ## # A tibble: 5 × 2 ## Bldg_Type n ## &lt;fct&gt; &lt;int&gt; ## 1 OneFam 2425 ## 2 TwoFmCon 62 ## 3 Duplex 109 ## 4 Twnhs 101 ## 5 TwnhsE 233 4.3.1 Dummy Variables One Hot Encoding (KNN, Tree Models) vs P-1 representation (Regression) One hot encoding is when all factor levels are included in the model. Adding in all P distinct levels along with the intercept term creates collinearity issues. model.matrix(~Bldg_Type -1, data = dat) %&gt;% head(1) ## Bldg_TypeOneFam Bldg_TypeTwoFmCon Bldg_TypeDuplex Bldg_TypeTwnhs ## 1 1 0 0 0 ## Bldg_TypeTwnhsE ## 1 0 P-1 encoding (Using all of the factor levels except the reference) R uses the first factor as the reference level and we should interpret the remaining levels relative to this factor. lm(Sale_Price ~ total_sf + Lot_Area + bath + Bedroom_AbvGr + Central_Air + Bldg_Type, data = dat) %&gt;% summary() %&gt;% broom::tidy() ## # A tibble: 10 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 1361. 4666. 0.292 7.71e- 1 ## 2 total_sf 100. 2.51 39.9 1.19e-278 ## 3 Lot_Area 0.430 0.118 3.65 2.68e- 4 ## 4 bath 28825. 1393. 20.7 7.17e- 89 ## 5 Bedroom_AbvGr -22921. 1325. -17.3 6.49e- 64 ## 6 Central_AirY 32779. 3661. 8.95 6.04e- 19 ## 7 Bldg_TypeTwoFmCon -38089. 6151. -6.19 6.77e- 10 ## 8 Bldg_TypeDuplex -47689. 4755. -10.0 2.71e- 23 ## 9 Bldg_TypeTwnhs -35466. 4856. -7.30 3.60e- 13 ## 10 Bldg_TypeTwnhsE -4281. 3489. -1.23 2.20e- 1 4.3.2 Ordered Factor Variables Treating ordered factors as a numerical variable preserves the information contained in the ordering that would be lost if we simply used a factor conversion (Likert scales, Loan grades, Crime rate, etc). "],["interpreting-regression-equations.html", "4.4 Interpreting Regression Equations", " 4.4 Interpreting Regression Equations 4.4.1 Correlated Variables (Variables that move together, either in the same direction or opposite direction) Remember back to our house sales data, the coefficient for Bedrooms was negative. This implies that the more bedrooms a house has, the less it will sell for. The reason is the total square feet and the number of bedrooms (and bathrooms) is highly correlated. We can see this using a Gaussian Graphical Model dat %&gt;% dplyr::select(total_sf, Lot_Area, bath, Bedroom_AbvGr) %&gt;% correlation::correlation(partial = TRUE) %&gt;% plot() ## Registered S3 methods overwritten by &#39;parameters&#39;: ## method from ## as.double.parameters_kurtosis datawizard ## as.double.parameters_skewness datawizard ## as.double.parameters_smoothness datawizard ## as.numeric.parameters_kurtosis datawizard ## as.numeric.parameters_skewness datawizard ## as.numeric.parameters_smoothness datawizard ## print.parameters_distribution datawizard ## print.parameters_kurtosis datawizard ## print.parameters_skewness datawizard ## summary.parameters_kurtosis datawizard ## summary.parameters_skewness datawizard ## Package &#39;ggraph&#39; needs to be loaded. Please load it by typing &#39;library(ggraph)&#39; into the console. ## NULL When we remove the total square feet and number of bathrooms, the number of bedrooms becomes desirable. We are essentially using this as a proxy for the size of the home. lm(Sale_Price ~ Lot_Area + Bedroom_AbvGr + Central_Air, data = dat) %&gt;% summary() %&gt;% broom::tidy() ## # A tibble: 4 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 53396. 7027. 7.60 3.99e-14 ## 2 Lot_Area 2.43 0.175 13.9 1.72e-42 ## 3 Bedroom_AbvGr 9956. 1666. 5.98 2.57e- 9 ## 4 Central_AirY 79630. 5475. 14.5 2.50e-46 4.4.2 Multicollinearity (when a predictor can be expressed as a linear combination of other predictors–extreme case of correlated variables) lm(Sale_Price ~ total_sf + First_Flr_SF + Second_Flr_SF + Lot_Area + Bedroom_AbvGr + Central_Air, data = dat) %&gt;% summary() %&gt;% broom::tidy() ## # A tibble: 6 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -16691. 4897. -3.41 6.62e- 4 ## 2 total_sf 108. 2.58 41.8 1.43e-299 ## 3 First_Flr_SF 47.7 2.93 16.3 3.61e- 57 ## 4 Lot_Area 0.194 0.120 1.61 1.08e- 1 ## 5 Bedroom_AbvGr -22774. 1294. -17.6 5.42e- 66 ## 6 Central_AirY 47527. 3598. 13.2 9.61e- 39 In the background, R handles this by removing a variable that causes multicollinearity (Second_Flr_SF). However, this is unstable and should be addressed explicitly. 4.4.3 Confounding Variables (problem of ommision) With our housing data, location may be a confounding (houses in some neighborhoods may sell at a higher price than other neighborhoods). neighborhood_groups &lt;- dat %&gt;% dplyr::mutate(resid = residuals(house_lm)) %&gt;% dplyr::group_by(Neighborhood) %&gt;% dplyr::summarize(med_resid = median(resid), cnt = n()) %&gt;% dplyr::arrange(med_resid) %&gt;% dplyr::mutate(cum_cnt = cumsum(cnt), neighborhoodgroup = forcats::as_factor(ntile(cum_cnt, 5))) dat &lt;- dat %&gt;% left_join(dplyr::select(neighborhood_groups, Neighborhood, neighborhoodgroup), by = &quot;Neighborhood&quot;) lm(Sale_Price ~ total_sf + Lot_Area + Bedroom_AbvGr + bath + Central_Air + neighborhoodgroup, data = dat) %&gt;% summary() %&gt;% broom::tidy() ## # A tibble: 10 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -5644. 4131. -1.37 1.72e- 1 ## 2 total_sf 86.7 2.26 38.4 3.83e-261 ## 3 Lot_Area 0.453 0.0990 4.58 4.94e- 6 ## 4 Bedroom_AbvGr -17093. 1074. -15.9 1.02e- 54 ## 5 bath 17560. 1222. 14.4 2.71e- 45 ## 6 Central_AirY 30157. 3071. 9.82 2.01e- 22 ## 7 neighborhoodgroup2 17332. 2615. 6.63 4.05e- 11 ## 8 neighborhoodgroup3 25081. 2546. 9.85 1.49e- 22 ## 9 neighborhoodgroup4 46051. 2733. 16.8 7.29e- 61 ## 10 neighborhoodgroup5 102744. 3289. 31.2 4.59e-185 4.4.4 Main Effects and Interactions interaction_fit &lt;- lm(Sale_Price ~ Lot_Area + Bedroom_AbvGr + bath + Central_Air + neighborhoodgroup*total_sf, data = dat) interaction_fit %&gt;% summary() %&gt;% broom::tidy() ## # A tibble: 14 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 14634. 6503. 2.25 2.45e- 2 ## 2 Lot_Area 0.497 0.0936 5.31 1.15e- 7 ## 3 Bedroom_AbvGr -16855. 1028. -16.4 7.90e-58 ## 4 bath 18578. 1158. 16.0 1.55e-55 ## 5 Central_AirY 33268. 2937. 11.3 3.95e-29 ## 6 neighborhoodgroup2 27831. 7329. 3.80 1.49e- 4 ## 7 neighborhoodgroup3 14349. 7390. 1.94 5.23e- 2 ## 8 neighborhoodgroup4 3273. 8054. 0.406 6.84e- 1 ## 9 neighborhoodgroup5 -40739. 9717. -4.19 2.84e- 5 ## 10 total_sf 68.0 4.38 15.5 2.85e-52 ## 11 neighborhoodgroup2:total_sf -5.95 4.94 -1.20 2.29e- 1 ## 12 neighborhoodgroup3:total_sf 7.12 5.17 1.38 1.69e- 1 ## 13 neighborhoodgroup4:total_sf 29.6 5.48 5.40 7.25e- 8 ## 14 neighborhoodgroup5:total_sf 76.6 5.49 13.9 7.29e-43 If an interaction is significant, it means the association is different at different levels of a factor or different values of a continuous variable. You will need to visually determine how this differs in order to interpret these results. interaction &lt;- ggeffects::ggpredict(interaction_fit, terms = c(&quot;neighborhoodgroup&quot;, &quot;total_sf&quot;)) plot(interaction) Selecting interaction terms Prior knowledge and intuition can guide choices Stepwise regression can be used to sift through variables Penalized regression can automatically fit to a large set of variables The most common approach is to use tree models, as well as their descendants which automatically search for optimal interaction terms. "],["regression-diagnostics.html", "4.5 Regression Diagnostics", " 4.5 Regression Diagnostics 4.5.1 Outliers (extreme value) This may not be an influential case Using the {performance} package by Daniel Lüdecke, we identify one influential case using Cook’s Distance (Other options are available). model &lt;- lm(Sale_Price ~ total_sf+ bath + Lot_Area + Bedroom_AbvGr, data = dat) outliers &lt;- performance::check_outliers(model) plot(outliers) as.data.frame(outliers) %&gt;% dplyr::arrange(-Outlier_Cook) %&gt;% head() ## Obs Distance_Cook Outlier_Cook Outlier ## 1 1499 1.178359e+00 1 1 ## 2 1 5.037865e-05 0 0 ## 3 2 2.824976e-07 0 0 ## 4 3 6.201489e-05 0 0 ## 5 4 1.308948e-04 0 0 ## 6 5 4.909392e-06 0 0 It is hard to tell whether this is a typo or a one off sale. The property sold for $160k but has an almost 64k lot area and over 5.6k square footage–quite a deal in this area. dat %&gt;% slice(1499) %&gt;% select(Sale_Price, total_sf, bath, Lot_Area, Bedroom_AbvGr) ## # A tibble: 1 × 5 ## Sale_Price total_sf bath Lot_Area Bedroom_AbvGr ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 160000 5642 4.5 63887 3 When we remove this influential case, our coefficients change quite a bit. house_noinfluence &lt;- lm(Sale_Price ~ total_sf+ bath + Lot_Area + Bedroom_AbvGr, data = dat %&gt;% slice(1:1498, 1500:n())) round(cbind(house_lm = model$coefficients, house_noinfluence = house_noinfluence$coefficients), digits = 3) ## house_lm house_noinfluence ## (Intercept) 28569.669 26596.755 ## total_sf 104.848 109.716 ## bath 28167.404 27124.051 ## Lot_Area 0.622 0.742 ## Bedroom_AbvGr -25683.007 -27089.435 4.5.2 Assumption Checking (Heteroscedasticity, Normality of residuals, Linearity, and Collinearity) performance::check_model(model) ## Loading required namespace: qqplotr "],["non-linear-regression.html", "4.6 Non-linear Regression", " 4.6 Non-linear Regression 4.6.1 Partial Residual Plots and Nonlinearity The linearity plot gives us some indication of a non-linear fit. To dig deeper, we can look at partial residual plots using the {ggeffects} package by Daniel Lüdecke. A partial residual plot represents the residuals of one dependent and one independent variable taking into account the other independent variables. Here is a standard scatterplot between Sales Price and Total Square Feet pr &lt;- ggeffects::ggpredict(model, &quot;total_sf [all]&quot;) plot(pr, add.data = TRUE) Here we produce a partial residual plot between Sales Price and Total Square Feet (taking into account the other independent variables). The blue line is a local polynomial regression line (loess) for reference. This indicates we may have a non-linear association. plot(pr, residuals = TRUE, residuals.line = TRUE) ## `geom_smooth()` using formula &#39;y ~ x&#39; 4.6.2 Polynomial and Spline Regression We can create a polynomial variable (predictor squared) and add it into the model. The polynomial model seems to more accurately represent these data. poly_model &lt;- lm(Sale_Price ~ poly(total_sf, 2) + bath + Lot_Area + Bedroom_AbvGr, data = dat) polynomial &lt;- ggeffects::ggpredict(poly_model, &quot;total_sf&quot;) plot(polynomial, residuals = TRUE, residuals.line = TRUE) ## `geom_smooth()` using formula &#39;y ~ x&#39; We can create a spline regression which will divides the dataset into multiple bins, called knots, and creates a separate fit for each bin. The difficult part is determining the correct knots. knots &lt;- quantile(dat$total_sf, p = c(0.25, 0.5, 0.75)) lm_spline &lt;- lm(Sale_Price ~ splines::bs(total_sf, knots = knots, degree = 3) + bath + Lot_Area + Bedroom_AbvGr, data = dat) spline &lt;- ggeffects::ggpredict(lm_spline, &quot;total_sf&quot;) plot(spline, residuals = TRUE, residuals.line = TRUE) ## `geom_smooth()` using formula &#39;y ~ x&#39; "],["generalized-additive-models.html", "4.7 Generalized Additive Models", " 4.7 Generalized Additive Models lm_gam &lt;- mgcv::gam(Sale_Price ~ s(total_sf) + bath + Lot_Area + Bedroom_AbvGr, data = dat) gam &lt;- ggeffects::ggpredict(lm_gam, &quot;total_sf&quot;) plot(gam, residuals = TRUE, residuals.line = TRUE) ## `geom_smooth()` using formula &#39;y ~ x&#39; "],["meeting-videos-4.html", "4.8 Meeting Videos", " 4.8 Meeting Videos 4.8.1 Cohort 1, Part 1 Meeting chat log 00:34:10 pavitra: https://www.programmingr.com/animation-graphics-r/qq-plot/#:~:text=The%20qqplot%20function%20in%20R.%201%20x%20is,is%20the%20name%20of%20the%20Q%20Q%20plot. 00:34:47 jiwan: maybe the base R plot() function on an lm result? it plots a bunch of things (resid, qq, …) 00:35:55 June Choe: re: qq plots - there&#39;s an interesting paper (and an R package {qqvases}) on QQ plots that draws an analogy to filling a vase with water which intuitively clicked for me - https://repository.upenn.edu/cgi/viewcontent.cgi?article=1605&amp;context=statistics_papers 00:36:35 pavitra: ooh..that looks good. Thanks June 00:36:49 pavitra: good catch, Jonathan 00:37:08 Ryan S: # to graph the residuals (from r4ds Ch25) library(modelr) library(tidyverse) library(gapminder) nz &lt;- filter(gapminder, country == &quot;New Zealand&quot;) nz_mod &lt;- lm(lifeExp ~ year, data = nz) plot3 &lt;- nz %&gt;% add_residuals(nz_mod) %&gt;% ggplot(aes(year, resid)) + geom_hline(yintercept = 0, colour = &quot;white&quot;, size = 3) + geom_line() + ggtitle(&quot;Resid pattern&quot;) 00:38:00 pavitra: nice! Thank you Ryan. That&#39;s what I wanted to know 00:40:02 pavitra: apparently bathrooms are not very significant in this model 00:40:04 pavitra: that figures 00:58:45 pavitra: thanks morgan. this was a dense chapter 00:58:54 June Choe: thanks for presenting! 00:58:59 priyanka gagneja: thx Morgan 00:59:10 Stan Piotrowski: Great presentation, Morgan! 00:59:12 jiwan: Thank you Morgan! 00:59:17 shamsuddeen: Thanks Morgan, great presentation 00:59:18 Morgan Grovenburg: Thank you all for your patience with me! 01:03:04 Andy Farina: Thank you Morgan, great presentation 01:04:07 Andy Farina: I am able to present if you would like a break 01:04:39 shamsuddeen: See yall 4.8.2 Cohort 1, Part 2 Meeting chat log no chat log "],["classification.html", "Chapter 5 Classification", " Chapter 5 Classification Learning objectives: Describe the general approach to binary classification. Use naive Bayes to predict a binary categorical variable from categorical predictors. Use linear discriminant analysis (LDA) to predict a binary categorical variable from normally distributed or categorical predictors. Use logistic regression to predict a binary categorical variable from predictors. Evaluate classification models. Deal with imbalanced data. "],["types-of-models.html", "5.1 Types of Models", " 5.1 Types of Models (we discussed these last week, but we don’t have slides yet) "],["evaluating-classification-models.html", "5.2 Evaluating Classification Models", " 5.2 Evaluating Classification Models Accuracy: \\(\\frac{TP+TN}{total}\\) yardstick::accuracy() Confusion matrix: columns = predictions, rows = actual, diagonal = correctly classified yardstick::conf_mat() Recall (aka Sensitivity): \\(\\frac{TP}{TP+FN}\\) “Of the true things, how many did the model ‘remember’?” yardstick::recall() Specificity: \\(\\frac{TN}{TN+FP}\\) “How good is the model at picking out bad things?” yardstick::spec() or yardstick::specificity() Precision: \\(\\frac{TP}{TP+FP}\\) “What portion of the predicted true things are true?” yardstick::precision() "],["roc-curves.html", "5.3 ROC Curves", " 5.3 ROC Curves Stands for “Receiver Operating Characteristics,” but that’s really just trivia. The book has the x-axis backwards, which is bonkers. yardstick::roc_curve constructs a tibble of data for the ROC curve, and can be autoplotted to generate the curve. library(yardstick) ## For binary classification, the first factor level is assumed to be the event. ## Use the argument `event_level = &quot;second&quot;` to alter this as needed. library(ggplot2) data(two_class_example) autoplot(roc_curve(two_class_example, truth, Class1)) AUC (yardstick::roc_auc()) = area under the ROC curve. 1 = perfect, 0.5 = random chance "],["lift.html", "5.4 Lift", " 5.4 Lift autoplot(lift_curve(two_class_example, truth, Class1)) "],["imbalanced-data.html", "5.5 Imbalanced Data", " 5.5 Imbalanced Data Undersample (aka downsample): Use fewer of the prevalent class (throw away data). Oversample (aka upsample): Bootstrap copies of the rare class. Up weighting and down weighting can do the ~same thing. Data generation (SMOTE) can be helpful to create cases similar to the rare class. "],["cost-based-classification.html", "5.6 Cost-Based Classification", " 5.6 Cost-Based Classification Rather than using straight-up accuracy or AUC, assign costs to false negatives and false positives, and classify based on cost. Barely touched on in the book, but I spent most of my slide-making time trying to think out how/whether we can use this. "],["meeting-videos-5.html", "5.7 Meeting Videos", " 5.7 Meeting Videos 5.7.1 Cohort 1, part 1 Meeting chat log 00:24:56 Jon Harmon (jonthegeek): To check out: StatQuest on YouTube. Josh Starmer? 00:25:07 Morgan Grovenburg: https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw 00:35:09 shamsuddeen: Thanks Jon 5.7.2 Cohort 1, part 2 Meeting chat log CHAT LOG "],["statistical-machine-learning.html", "Chapter 6 Statistical Machine Learning", " Chapter 6 Statistical Machine Learning Learning objectives: TBD "],["slide-2-1.html", "6.1 SLIDE 2", " 6.1 SLIDE 2 ADD SLIDE CONTENTS "],["slide-3-1.html", "6.2 SLIDE 3", " 6.2 SLIDE 3 MORE SLIDE CONTENTS "],["meeting-videos-6.html", "6.3 Meeting Videos", " 6.3 Meeting Videos 6.3.1 Cohort 1 Meeting chat log CHAT LOG "],["unsupervised-learning.html", "Chapter 7 Unsupervised Learning", " Chapter 7 Unsupervised Learning Learning objectives: TBD "],["slide-2-2.html", "7.1 SLIDE 2", " 7.1 SLIDE 2 ADD SLIDE CONTENTS "],["slide-3-2.html", "7.2 SLIDE 3", " 7.2 SLIDE 3 MORE SLIDE CONTENTS "],["meeting-videos-7.html", "7.3 Meeting Videos", " 7.3 Meeting Videos 7.3.1 Cohort 1 Meeting chat log CHAT LOG "]]
