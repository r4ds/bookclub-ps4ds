[["index.html", "Practical Statistics for Data Scientists Book Club Welcome", " Practical Statistics for Data Scientists Book Club The R4DS Online Learning Community 2021-07-19 Welcome This is a companion for the book Practical Statistics for Data Scientists by Peter Bruce, Andrew Bruce, and Peter Gedeck (O’Reilly, copyright 2020, 978-1-492-07294-2). This companion is available at r4ds.io/ps4ds. This website is being developed by the R4DS Online Learning Community. Follow along, and join the community to participate. This companion follows the R4DS Online Learning Community Code of Conduct. "],["book-club-meetings.html", "Book club meetings", " Book club meetings Each week, a volunteer will present a chapter from the book (or part of a chapter). This is the best way to learn the material. Presentations will usually consist of a review of the material, a discussion, and/or a demonstration of the principles presented in that chapter. More information about how to present is available in the github repo. Presentations will be recorded, and will be available on the R4DS Online Learning Community YouTube Channel. "],["sample-code.html", "Sample code", " Sample code Sample code is available at github.com/gedeck/practical-statistics-for-data-scientists You can install all packages used by these notes and recommended by the book authors1: install.packages(&quot;remotes&quot;) remotes::install_github(&quot;r4ds/bookclub-ps4ds&quot;) remove.packages(&quot;ps4ds&quot;) # This isn&#39;t really a package. Chapter 5 uses {DMwR}, which is currently unavailable on CRAN. We’ll confirm when we get there, but I’m guessing {DMwR2} will work instead.↩︎ "],["st-edition-vs-2nd-edition.html", "1st edition vs 2nd edition", " 1st edition vs 2nd edition Here we’ll attempt to document differences between 1e and 2e. 2e added Python example code Some sections and subsections have slight name changes Chapter 1: New subsection in 1.6, “Probability” Chapter 2: New sections “Chi-Square Distribution” &amp; “F-Distribution” Chapter 4: New subsection in 4.2, “Further Reading” Chapter 7: New subsection in 7.1, “Correspondence Analysis” "],["pace.html", "Pace", " Pace Chapters are long, but not always dense. We’ll try to cover 1 chapter/week, but… …It’s ok to split chapters when they feel like too much. "],["exploratory-data-analysis.html", "Chapter 1 Exploratory Data Analysis", " Chapter 1 Exploratory Data Analysis Learning objectives: Classify data as numeric or categorical. Compare and contrast estimates of location. Compare and contrast estimates of variability. Visualize data distributions. Visualize categorical data. Use correlation coefficients to measure association between two variables. Visualize data distributions in two dimensions. "],["structured-data.html", "1.1 Structured Data", " 1.1 Structured Data Software classifies data by type. Numeric (continuous or discrete) Categorical (binary, ordinal, neither) Rectangular data = typical frame of reference for data science. Called a data.frame in R Rows are records (aka observations, cases, instances) Columns are features (aka variables, attributes, predictors in some cases) Lots of synonyms in stats and data science for same things. "],["estimates-of-location.html", "1.2 Estimates of Location", " 1.2 Estimates of Location Most basic = mean. dataset &lt;- c(3, 4, 1, 2, 10) mean(dataset) # (3 + 4 + 1 + 2 + 10)/5 = 20/5 ## [1] 4 Trimming helps eliminate outliers mean(dataset, trim = 1/5) # (2 + 3 + 4)/3 = 9/3 ## [1] 3 Weight to: Down-weight high-variability values. Up-weight under-represented values. weights &lt;- c(1, 1, 11, 1, 1) weighted.mean(dataset, weights) # (3 + 4 + 11 + 2 + 10)/15 = 30/15 ## [1] 2 Median: sort then choose middle value. median(dataset) # 1, 2, (3), 4, 10 ## [1] 3 Weighted median: similar to weighted mean, but more complicated. # Sort then weight then middle of weight. 1*11, 2*1, 3*1, 4*1, 10*1 matrixStats::weightedMedian(dataset, weights) ## [1] 1.333333 Technically it interpolates in-between values. matrixStats::weightedMedian(dataset, weights, interpolate = TRUE) ## [1] 1.333333 Can tell it not to interpolate to simplify. matrixStats::weightedMedian(dataset, weights, interpolate = FALSE) ## [1] 1 # Equivalent to repeating values weight times. median(c(rep(1, 11), 2, 3, 4, 10)) ## [1] 1 Their sample code is available at github.com/gedeck/practical-statistics-for-data-scientists "],["estimates-of-variability.html", "1.3 Estimates of Variability", " 1.3 Estimates of Variability Variability (aka dispersion) = are values clustered or spread out? 1.3.1 SD &amp; Friends Variance = average of squared deviations, \\(s^2 = \\frac{\\sum_{i=1}^{n}{(x_{1}-\\bar{x})^2}}{n-1}\\) s_squared &lt;- var(dataset) s_squared ## [1] 12.5 Standard deviation = square root of variance, \\(s = \\sqrt{variance}\\) s &lt;- sd(dataset) s ## [1] 3.535534 s == sqrt(s_squared) ## [1] TRUE Median absolute deviation from the median (MAD) is robust to outliers. mad(dataset) ## [1] 1.4826 Wait, why did that return the standard scale factor? dataset is c(1, 2, 3, 4, 10) The difference between any 2 values is 1 (except the outlier) 1 * 1.4826 = 1.4826 1.3.2 Percentiles &amp; Friends Percentiles = quantiles, \\(P\\%\\) of values are \\(&lt;= x\\) x &lt;- sample(1:100, 100, replace = TRUE) y &lt;- rnorm(100, mean = 50, sd = 20) quantile(x, probs = seq(0, 1, 0.1)) ## 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% ## 2.0 12.0 19.0 24.0 39.0 51.5 58.4 69.6 76.0 91.0 100.0 quantile(y, probs = seq(0, 1, 0.1)) ## 0% 10% 20% 30% 40% 50% 60% 70% ## 12.06662 21.74080 34.96941 39.43207 46.25191 52.02947 57.14075 62.55494 ## 80% 90% 100% ## 68.80972 75.50027 107.68776 quantile(x) # quartile ## 0% 25% 50% 75% 100% ## 2.0 22.5 51.5 73.0 100.0 IQR(x) # They introduce this later but I like it here. ## [1] 50.5 "],["histograms-friends.html", "1.4 Histograms &amp; Friends", " 1.4 Histograms &amp; Friends state &lt;- read.csv(&quot;data/state.csv&quot;) head(state) ## State Population Murder.Rate Abbreviation ## 1 Alabama 4779736 5.7 AL ## 2 Alaska 710231 5.6 AK ## 3 Arizona 6392017 4.7 AZ ## 4 Arkansas 2915918 5.6 AR ## 5 California 37253956 4.4 CA ## 6 Colorado 5029196 2.8 CO library(ggplot2) ggplot(state, aes(y = Population/1000000)) + geom_boxplot() + ylab(&quot;Population (millions)&quot;) ggplot(state, aes(x = Population/1000000)) + geom_histogram( aes(y = after_stat(density)), bins = 10, fill = &quot;white&quot;, color = &quot;black&quot; ) + geom_density(fill = &quot;blue&quot;, alpha = 0.5) + xlab(&quot;Population (millions)&quot;) "],["visualizing-categorical-data.html", "1.5 Visualizing Categorical Data", " 1.5 Visualizing Categorical Data Bar charts are boring. We’ll see some examples related to this in 2D. "],["correlation.html", "1.6 Correlation", " 1.6 Correlation library(corrplot) ## corrplot 0.90 loaded library(dplyr, quietly = TRUE) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union sp500_px &lt;- read.csv(&quot;data/sp500_data.csv.gz&quot;) %&gt;% as_tibble() sp500_sym &lt;- read.csv(&quot;data/sp500_sectors.csv&quot;, stringsAsFactors = FALSE) %&gt;% as_tibble() etfs &lt;- sp500_px %&gt;% filter(X &gt; &quot;2012-07-01&quot;) %&gt;% select( any_of( sp500_sym %&gt;% filter(sector == &quot;etf&quot;) %&gt;% pull(symbol) ) ) corrplot(cor(etfs), method = &quot;ellipse&quot;) "],["d-distributions.html", "1.7 2D Distributions", " 1.7 2D Distributions https://xkcd.com/1967/ "],["meeting-videos.html", "1.8 Meeting Videos", " 1.8 Meeting Videos 1.8.1 Cohort 1 Meeting chat log CHAT LOG "],["data-and-sampling-distributions.html", "Chapter 2 Data and Sampling Distributions", " Chapter 2 Data and Sampling Distributions Learning objectives: Understand how to sample from a population. Identify various kinds of sampling bias. Know how to avoid bias in sampling. Understand the distribution of a sample statistic. Use the bootstrap to quantify sampling variability. Calculate confidence intervals. Recognize the most important common distributions. "],["what-is-a-population.html", "2.1 What is a Population?", " 2.1 What is a Population? roughly, “a particular set of things we care about” may be concrete the set of people who will vote in the next election all the trees in some forest more generally, is notional the space of outcomes from rolling a pair of dice all possible offspring from a mating pair of fruit flies the collection of physical microstates consistent with a given macrostate etc. "],["populations.html", "Populations", " Populations A population is represented as a distribution over one or more variables. voting populations are a distribution over the candidates tree populations are a distribution over the species of tree, the diameter of the trunk, the number of leaves, the thickness of the bark, etc. dice outcomes are a distribution over the number rolled etc. "],["population-statistics.html", "Population Statistics", " Population Statistics The things we care about are statistics that can be calculated from the distribution. the mode of the candidate distribution the median height of the trees, divided by the MAD of the number of branchings2 the mean and standard deviation of the number rolled etc. I am sure that nobody cares about this metric. The point is that a “statistic” can be any function of the distribution.↩︎ "],["what-is-a-sample.html", "2.2 What is a Sample?", " 2.2 What is a Sample? We almost never have full access to the population distribution that we care about, so we have to settle for a sample. consists of some number n of “individuals” from the population poll 2000 likely voters randomly select 50 trees from the forest to measure roll the dice 100 times drawn at random from the population represented by a distribution over the same variables as the population Whatever statistic we wanted to calculate for the population, we instead calculate for the sample. "],["what-is-a-sample-1.html", "What is a Sample?", " What is a Sample? The book gives the url for a helpful demo: https://onlinestatbook.com/stat_sim/sampling_dist/ "],["we-have-a-problem.html", "2.3 We Have a Problem", " 2.3 We Have a Problem The sample is not the population! The sample statistics we calculate are not equal to the population statistics! The sample statistic may differ from the population statistic for a variety of reasons: random fluctuation bias selection bias (the sample may not have been drawn randomly from the population) sample size bias3 (some sample metrics will be inherently and systematically different from population metrics just because of the limited size of the sample) the book glosses over this, so I don’t know if there’s a more standard term than just “bias”↩︎ "],["the-ideal-solution.html", "2.4 The Ideal Solution", " 2.4 The Ideal Solution If we had the resources to take many samples (e.g. 100 other research groups doing the same study of the forest that we are), then we could do the following: Repeat the sampling process some number of times, taking a new random sample each time. For each sample, calculate the sample statistic. Make a histogram of all the resulting values for the sample statistic. "],["the-ideal-solution-1.html", "The Ideal Solution", " The Ideal Solution The resulting sampling distribution of the statistic would help us understand the results of our sampling experiment. The mean of the sampling distribution is (an estimate of) the value of the statistic that our experiment is “aiming at.”4 The standard deviation of the distribution is (an estimate of) how much random fluctuations are likely to influence our measurement. This is also known as the standard error of our calculated sample statistic. This may not be equal to the population statistic, due to sample size bias. For example, if my target statistic is the range of some variable, the sample statistic will always be less than or equal to the population statistic. And so the mean of the sampling distribution for the range will be less than the population range. Some statistics, like the mean, are known to be unbiased statistics, while others are known to be biased.↩︎ "],["the-central-limit-theorem.html", "2.5 The Central Limit Theorem", " 2.5 The Central Limit Theorem In the special case where our sample statistic is the mean, it can be shown that: the sampling distribution approaches a normal distribution. an estimate of the standard deviation of that distribution is given by the standard deviation of an individual sample, divided by the square root of the sample size. "],["the-bootstrap-solution.html", "2.6 The Bootstrap Solution", " 2.6 The Bootstrap Solution If our sample statistic is the mean5, we can estimate the standard error from a single sample. Otherwise, to measure the standard error of our sample statistic, we would have to repeat our sampling process many times to be able to calculate the standard deviation of the sampling distribution. (Of course, if we actually did this, our calculated standard error would be pretty useless, because we’d have a better estimate of the statistic from the combined samples.) In practice, we can use a bootstrap. or other statistic for which a theoretical approximation to the standard error has been derived↩︎ "],["the-bootstrap.html", "The Bootstrap", " The Bootstrap The bootstrap is a simple but powerful technique for estimating the standard error of any sample statistic from a single sample. We will also obtain an estimate of the sample size bias for our sample statistic. Sounds too good to be true. Where does this amazing capability come from? "],["the-bootstrap-1.html", "The Bootstrap", " The Bootstrap The bootstrap works like this: Assume that the sample distribution is representative of the population distribution.6 Construct a new simulated population by making a gazillion copies of your sample (in practice, this just means you draw from the sample with replacement). Now it is easy to do lots of simulated experiments on your simulated population! Draw lots of bootstrap samples from your simulated population, with each sample having the same n as your original sample. Calculate your sample statistic for each of the bootstrap samples. Calculate the standard deviation of the resulting distribution (of bootstrapped sample statistics); this is the estimated standard error for your measured sample statistic. Calculate the difference between the mean of the bootstrap distribution and the statistic calculated on your original sample; this is the estimated bias for your measurement. We can use the same sampling demo to get a feel for how bootstrapping works. It doesn’t have to be perfect, but if it’s too far off then nothing you calculate from the sample will be meaningful anyway.↩︎ "],["the-bootstrap-limitations.html", "The Bootstrap: Limitations", " The Bootstrap: Limitations The bootstrap isn’t magic. It won’t give you a better estimate of your sample statistic.7 It won’t fix selection bias. It won’t fill gaps in your sample data. It does help you understand the limitations of your experimental procedure. Well, having an estimate of the sample size bias can help.↩︎ "],["confidence-intervals.html", "2.7 Confidence Intervals", " 2.7 Confidence Intervals A confidence interval, like the standard error, is a way to estimate the reliability of a sample statistic. For example, a 95% CI is an interval that would contain the central 95% of values for the sample statistic, if the sampling experiment were done a very large number of times. It’s generally not practical to actually sample that many times, so… bootstrap! For comparison, plus or minus one SE gives a CI of about 68%. "],["some-important-distributions.html", "2.8 Some Important Distributions", " 2.8 Some Important Distributions The rest of the chapter is about specific distributions. Many of these distributions will come up again in particular contexts in following chapters. "],["the-normal-distribution.html", "2.9 The Normal Distribution", " 2.9 The Normal Distribution normal_values &lt;- rnorm(n = 10000) hist(normal_values, breaks = 30) # qqnorm plots the location of quantiles of the given distribution # vs locations of corresponding quantiles of normal distribution. qqnorm(normal_values); qqline(normal_values, col = 2) "],["students-t-distribution.html", "2.10 Student’s t-Distribution", " 2.10 Student’s t-Distribution # t-distribution is a family parameterized by degrees of freedom t_values &lt;- rt(n = 10000, df = 10) hist(t_values, breaks = 30) # normalize distribution for QQ t_values &lt;- (t_values - mean(t_values))/sd(t_values) qqnorm(t_values); qqline(t_values, col = 2) "],["the-binomial-distribution.html", "2.11 The Binomial Distribution", " 2.11 The Binomial Distribution # flipping 6 fair coins at a time, how many heads do we get? binom_values &lt;- rbinom(n = 10000, 6, 0.5) hist(binom_values, breaks = 30) # normalize distribution for QQ binom_values &lt;- (binom_values - mean(binom_values))/sd(binom_values) qqnorm(binom_values); qqline(binom_values, col = 2) "],["the-chi-square-distribution.html", "2.12 The Chi-Square Distribution", " 2.12 The Chi-Square Distribution # family of distributions parameterized by degrees of freedom chisq_values &lt;- rchisq(n = 10000, df = 5) hist(chisq_values, breaks = 30) # normalize distribution for QQ chisq_values &lt;- (chisq_values - mean(chisq_values))/sd(chisq_values) qqnorm(chisq_values); qqline(chisq_values, col = 2) "],["the-f-distribution.html", "2.13 The F-Distribution", " 2.13 The F-Distribution # family of distributions parameterized by TWO df1 values f_values &lt;- rf(n = 10000, df1 = 15, df2 = 50) hist(f_values, breaks = 30) # normalize distribution for QQ f_values &lt;- (f_values - mean(f_values))/sd(f_values) qqnorm(f_values); qqline(f_values, col = 2) "],["the-poisson-distribution.html", "2.14 The Poisson Distribution", " 2.14 The Poisson Distribution # family of distributions parameterized by lambda (&quot;mean rate&quot;) poisson_values &lt;- rpois(n = 10000, lambda = 5) hist(poisson_values, breaks = 30) # normalize distribution for QQ poisson_values &lt;- (poisson_values - mean(poisson_values))/sd(poisson_values) qqnorm(poisson_values); qqline(poisson_values, col = 2) "],["meeting-videos-1.html", "2.15 Meeting Videos", " 2.15 Meeting Videos 2.15.1 Cohort 1 Meeting chat log CHAT LOG "],["statistical-experiments-and-significance-testing.html", "Chapter 3 Statistical Experiments and Significance Testing", " Chapter 3 Statistical Experiments and Significance Testing Main point: resampling is the best way to test the significance of a statistical experiment. Learning objectives: Design A/B tests. Use hypothesis tests to understand the results of a statistical experiment. Perform resampling procedures to test the significance of statistical experiments. Explain the proper uses (and abuses!) of p-values. Compare and contrast the various traditional tests of significance. Explain the advantages of multi-armed bandit tests over traditional A/B tests. Calculate the statistical power for a statistical experiment. "],["slide-2.html", "3.1 SLIDE 2", " 3.1 SLIDE 2 ADD SLIDE CONTENTS "],["slide-3.html", "3.2 SLIDE 3", " 3.2 SLIDE 3 MORE SLIDE CONTENTS "],["meeting-videos-2.html", "3.3 Meeting Videos", " 3.3 Meeting Videos 3.3.1 Cohort 1 Meeting chat log CHAT LOG "],["regression-and-prediction.html", "Chapter 4 Regression and Prediction", " Chapter 4 Regression and Prediction Learning objectives: Perform linear regressions with a single independent variable. Perform linear regressions with multiple independent variables. Perform regressions with one or more categorical independent variables. Perform nonlinear generalizations of regression. Compare and contrast the use of regression for prediction vs. explanation. Cautiously interpret the results of a multivariable linear regression. Assess the goodness of a regression model. "],["weighted-regression.html", "4.1 Weighted Regression", " 4.1 Weighted Regression Used to give certain records (variables, features) more or less weighting when fitting the regression model. To show this, I will use the ames housing data from the {modeldata} package from tidymodels and prioritize sale prices of houses sold more recently than those sold earlier in these data. dat &lt;- ames %&gt;% dplyr::select(Lot_Area, Neighborhood, Year_Sold, First_Flr_SF, Second_Flr_SF, Bsmt_Full_Bath, Full_Bath, Half_Bath, Bsmt_Half_Bath, Sale_Price, Bedroom_AbvGr, Central_Air, Bldg_Type) %&gt;% dplyr::mutate(weight = Year_Sold - 2006, total_sf = First_Flr_SF + Second_Flr_SF, bath = Bsmt_Full_Bath + Full_Bath + 0.5*Half_Bath + 0.5*Bsmt_Half_Bath) house_lm &lt;- lm(Sale_Price ~ total_sf + Lot_Area + bath + Bedroom_AbvGr + Central_Air, data = dat) house_wt &lt;- lm(Sale_Price ~ total_sf + Lot_Area + bath + Bedroom_AbvGr + Central_Air, data = dat, weight = weight) round(cbind(house_lm = house_lm$coefficients, house_wt = house_wt$coefficients), digits = 3) ## house_lm house_wt ## (Intercept) -4804.696 -3943.203 ## total_sf 104.742 104.705 ## Lot_Area 0.605 0.673 ## bath 25411.278 24754.042 ## Bedroom_AbvGr -25438.440 -27218.685 ## Central_AirY 41924.976 45938.683 "],["prediction-using-regression.html", "4.2 Prediction using Regression", " 4.2 Prediction using Regression Caution: Be careful extrapolating results beyond the range of the dataset Prediction Interval (Uncertainty around a single value) Confidence Intervals (Uncertainty around a statistic) Individual data point error Here is an example of individual data point error. If we filter for those properties that have four bedrooms, 3 bathrooms and a lot square footage between 10k and 11k, the sale price varies by $50k. This is error in our model. dat %&gt;% dplyr::filter(Bedroom_AbvGr == 4 &amp; bath == 3 &amp; Lot_Area &gt;=10000 &amp; Lot_Area &lt; 11000) %&gt;% dplyr::select(Sale_Price) %&gt;% dplyr::arrange(-Sale_Price) ## # A tibble: 9 x 1 ## Sale_Price ## &lt;int&gt; ## 1 218500 ## 2 211000 ## 3 170000 ## 4 165150 ## 5 157000 ## 6 139000 ## 7 127500 ## 8 103500 ## 9 100000 "],["factor-variables.html", "4.3 Factor Variables", " 4.3 Factor Variables We can use the building type as a factor variable to help with our predictions. The building type variable has five options: dat %&gt;% count(Bldg_Type) ## # A tibble: 5 x 2 ## Bldg_Type n ## &lt;fct&gt; &lt;int&gt; ## 1 OneFam 2425 ## 2 TwoFmCon 62 ## 3 Duplex 109 ## 4 Twnhs 101 ## 5 TwnhsE 233 4.3.1 Dummy Variables One Hot Encoding (KNN, Tree Models) vs P-1 representation (Regression) One hot encoding is when all factor levels are included in the model. Adding in all P distinct levels along with the intercept term creates collinearity issues. model.matrix(~Bldg_Type -1, data = dat) %&gt;% head(1) ## Bldg_TypeOneFam Bldg_TypeTwoFmCon Bldg_TypeDuplex Bldg_TypeTwnhs ## 1 1 0 0 0 ## Bldg_TypeTwnhsE ## 1 0 P-1 encoding (Using all of the factor levels except the reference) R uses the first factor as the reference level and we should interpret the remaining levels relative to this factor. lm(Sale_Price ~ total_sf + Lot_Area + bath + Bedroom_AbvGr + Central_Air + Bldg_Type, data = dat) %&gt;% summary() %&gt;% broom::tidy() ## # A tibble: 10 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 1361. 4666. 0.292 7.71e- 1 ## 2 total_sf 100. 2.51 39.9 1.19e-278 ## 3 Lot_Area 0.430 0.118 3.65 2.68e- 4 ## 4 bath 28825. 1393. 20.7 7.17e- 89 ## 5 Bedroom_AbvGr -22921. 1325. -17.3 6.49e- 64 ## 6 Central_AirY 32779. 3661. 8.95 6.04e- 19 ## 7 Bldg_TypeTwoFmCon -38089. 6151. -6.19 6.77e- 10 ## 8 Bldg_TypeDuplex -47689. 4755. -10.0 2.71e- 23 ## 9 Bldg_TypeTwnhs -35466. 4856. -7.30 3.60e- 13 ## 10 Bldg_TypeTwnhsE -4281. 3489. -1.23 2.20e- 1 4.3.2 Ordered Factor Variables Treating ordered factors as a numerical variable preserves the information contained in the ordering that would be lost if we simply used a factor conversion (Likert scales, Loan grades, Crime rate, etc). "],["interpreting-regression-equations.html", "4.4 Interpreting Regression Equations", " 4.4 Interpreting Regression Equations 4.4.1 Correlated Variables (Variables that move together, either in the same direction or opposite direction) Remember back to our house sales data, the coefficient for Bedrooms was negative. This implies that the more bedrooms a house has, the less it will sell for. The reason is the total square feet and the number of bedrooms (and bathrooms) is highly correlated. We can see this using a Gaussian Graphical Model dat %&gt;% dplyr::select(total_sf, Lot_Area, bath, Bedroom_AbvGr) %&gt;% correlation::correlation(partial = TRUE) %&gt;% plot() ## Package &#39;ggraph&#39; needs to be loaded. Please load it by typing &#39;library(ggraph)&#39; into the console. ## NULL When we remove the total square feet and number of bathrooms, the number of bedrooms becomes desirable. We are essentially using this as a proxy for the size of the home. lm(Sale_Price ~ Lot_Area + Bedroom_AbvGr + Central_Air, data = dat) %&gt;% summary() %&gt;% broom::tidy() ## # A tibble: 4 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 53396. 7027. 7.60 3.99e-14 ## 2 Lot_Area 2.43 0.175 13.9 1.72e-42 ## 3 Bedroom_AbvGr 9956. 1666. 5.98 2.57e- 9 ## 4 Central_AirY 79630. 5475. 14.5 2.50e-46 4.4.2 Multicollinearity (when a predictor can be expressed as a linear combination of other predictors–extreme case of correlated variables) lm(Sale_Price ~ total_sf + First_Flr_SF + Second_Flr_SF + Lot_Area + Bedroom_AbvGr + Central_Air, data = dat) %&gt;% summary() %&gt;% broom::tidy() ## # A tibble: 6 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -16691. 4897. -3.41 6.62e- 4 ## 2 total_sf 108. 2.58 41.8 1.43e-299 ## 3 First_Flr_SF 47.7 2.93 16.3 3.61e- 57 ## 4 Lot_Area 0.194 0.120 1.61 1.08e- 1 ## 5 Bedroom_AbvGr -22774. 1294. -17.6 5.42e- 66 ## 6 Central_AirY 47527. 3598. 13.2 9.61e- 39 In the background, R handles this by removing a variable that causes multicollinearity (Second_Flr_SF). However, this is unstable and should be addressed explicitly. 4.4.3 Confounding Variables (problem of ommision) With our housing data, location may be a confounding (houses in some neighborhoods may sell at a higher price than other neighborhoods). neighborhood_groups &lt;- dat %&gt;% dplyr::mutate(resid = residuals(house_lm)) %&gt;% dplyr::group_by(Neighborhood) %&gt;% dplyr::summarize(med_resid = median(resid), cnt = n()) %&gt;% dplyr::arrange(med_resid) %&gt;% dplyr::mutate(cum_cnt = cumsum(cnt), neighborhoodgroup = forcats::as_factor(ntile(cum_cnt, 5))) dat &lt;- dat %&gt;% left_join(dplyr::select(neighborhood_groups, Neighborhood, neighborhoodgroup), by = &quot;Neighborhood&quot;) lm(Sale_Price ~ total_sf + Lot_Area + Bedroom_AbvGr + bath + Central_Air + neighborhoodgroup, data = dat) %&gt;% summary() %&gt;% broom::tidy() ## # A tibble: 10 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -5644. 4131. -1.37 1.72e- 1 ## 2 total_sf 86.7 2.26 38.4 3.83e-261 ## 3 Lot_Area 0.453 0.0990 4.58 4.94e- 6 ## 4 Bedroom_AbvGr -17093. 1074. -15.9 1.02e- 54 ## 5 bath 17560. 1222. 14.4 2.71e- 45 ## 6 Central_AirY 30157. 3071. 9.82 2.01e- 22 ## 7 neighborhoodgroup2 17332. 2615. 6.63 4.05e- 11 ## 8 neighborhoodgroup3 25081. 2546. 9.85 1.49e- 22 ## 9 neighborhoodgroup4 46051. 2733. 16.8 7.29e- 61 ## 10 neighborhoodgroup5 102744. 3289. 31.2 4.59e-185 4.4.4 Main Effects and Interactions interaction_fit &lt;- lm(Sale_Price ~ Lot_Area + Bedroom_AbvGr + bath + Central_Air + neighborhoodgroup*total_sf, data = dat) interaction_fit %&gt;% summary() %&gt;% broom::tidy() ## # A tibble: 14 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 14634. 6503. 2.25 2.45e- 2 ## 2 Lot_Area 0.497 0.0936 5.31 1.15e- 7 ## 3 Bedroom_AbvGr -16855. 1028. -16.4 7.90e-58 ## 4 bath 18578. 1158. 16.0 1.55e-55 ## 5 Central_AirY 33268. 2937. 11.3 3.95e-29 ## 6 neighborhoodgroup2 27831. 7329. 3.80 1.49e- 4 ## 7 neighborhoodgroup3 14349. 7390. 1.94 5.23e- 2 ## 8 neighborhoodgroup4 3273. 8054. 0.406 6.84e- 1 ## 9 neighborhoodgroup5 -40739. 9717. -4.19 2.84e- 5 ## 10 total_sf 68.0 4.38 15.5 2.85e-52 ## 11 neighborhoodgroup2:total_sf -5.95 4.94 -1.20 2.29e- 1 ## 12 neighborhoodgroup3:total_sf 7.12 5.17 1.38 1.69e- 1 ## 13 neighborhoodgroup4:total_sf 29.6 5.48 5.40 7.25e- 8 ## 14 neighborhoodgroup5:total_sf 76.6 5.49 13.9 7.29e-43 If an interaction is significant, it means the association is different at different levels of a factor or different values of a continuous variable. You will need to visually determine how this differs in order to interpret these results. interaction &lt;- ggeffects::ggpredict(interaction_fit, terms = c(&quot;neighborhoodgroup&quot;, &quot;total_sf&quot;)) plot(interaction) Selecting interaction terms Prior knowledge and intuition can guide choices Stepwise regression can be used to sift through variables Penalized regression can automatically fit to a large set of variables The most common approach is to use tree models, as well as their descendants which automatically search for optimal interaction terms. "],["regression-diagnostics.html", "4.5 Regression Diagnostics", " 4.5 Regression Diagnostics 4.5.1 Outliers (extreme value) This may not be an influential case Using the {performance} package by Daniel Lüdecke, we identify one influential case using Cook’s Distance (Other options are available). model &lt;- lm(Sale_Price ~ total_sf+ bath + Lot_Area + Bedroom_AbvGr, data = dat) outliers &lt;- performance::check_outliers(model) plot(outliers) as.data.frame(outliers) %&gt;% dplyr::arrange(-Outlier_Cook) %&gt;% head() ## Obs Distance_Cook Outlier_Cook Outlier ## 1 1499 1.178359e+00 1 1 ## 2 1 5.037865e-05 0 0 ## 3 2 2.824976e-07 0 0 ## 4 3 6.201489e-05 0 0 ## 5 4 1.308948e-04 0 0 ## 6 5 4.909392e-06 0 0 It is hard to tell whether this is a typo or a one off sale. The property sold for $160k but has an almost 64k lot area and over 5.6k square footage–quite a deal in this area. dat %&gt;% slice(1499) %&gt;% select(Sale_Price, total_sf, bath, Lot_Area, Bedroom_AbvGr) ## # A tibble: 1 x 5 ## Sale_Price total_sf bath Lot_Area Bedroom_AbvGr ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 160000 5642 4.5 63887 3 When we remove this influential case, our coefficients change quite a bit. house_noinfluence &lt;- lm(Sale_Price ~ total_sf+ bath + Lot_Area + Bedroom_AbvGr, data = dat %&gt;% slice(1:1498, 1500:n())) round(cbind(house_lm = model$coefficients, house_noinfluence = house_noinfluence$coefficients), digits = 3) ## house_lm house_noinfluence ## (Intercept) 28569.669 26596.755 ## total_sf 104.848 109.716 ## bath 28167.404 27124.051 ## Lot_Area 0.622 0.742 ## Bedroom_AbvGr -25683.007 -27089.435 4.5.2 Assumption Checking (Heteroscedasticity, Normality of residuals, Linearity, and Collinearity) performance::check_model(model) ## Loading required namespace: qqplotr ## For confidence bands, please install `qqplotr`. "],["non-linear-regression.html", "4.6 Non-linear Regression", " 4.6 Non-linear Regression 4.6.1 Partial Residual Plots and Nonlinearity The linearity plot gives us some indication of a non-linear fit. To dig deeper, we can look at partial residual plots using the {ggeffects} package by Daniel Lüdecke. A partial residual plot represents the residuals of one dependent and one independent variable taking into account the other independent variables. Here is a standard scatterplot between Sales Price and Total Square Feet pr &lt;- ggeffects::ggpredict(model, &quot;total_sf [all]&quot;) plot(pr, add.data = TRUE) Here we produce a partial residual plot between Sales Price and Total Square Feet (taking into account the other independent variables). The blue line is a local polynomial regression line (loess) for reference. This indicates we may have a non-linear association. plot(pr, residuals = TRUE, residuals.line = TRUE) ## `geom_smooth()` using formula &#39;y ~ x&#39; 4.6.2 Polynomial and Spline Regression We can create a polynomial variable (predictor squared) and add it into the model. The polynomial model seems to more accurately represent these data. poly_model &lt;- lm(Sale_Price ~ poly(total_sf, 2) + bath + Lot_Area + Bedroom_AbvGr, data = dat) polynomial &lt;- ggeffects::ggpredict(poly_model, &quot;total_sf&quot;) plot(polynomial, residuals = TRUE, residuals.line = TRUE) ## `geom_smooth()` using formula &#39;y ~ x&#39; We can create a spline regression which will divides the dataset into multiple bins, called knots, and creates a separate fit for each bin. The difficult part is determining the correct knots. knots &lt;- quantile(dat$total_sf, p = c(0.25, 0.5, 0.75)) lm_spline &lt;- lm(Sale_Price ~ splines::bs(total_sf, knots = knots, degree = 3) + bath + Lot_Area + Bedroom_AbvGr, data = dat) spline &lt;- ggeffects::ggpredict(lm_spline, &quot;total_sf&quot;) plot(spline, residuals = TRUE, residuals.line = TRUE) ## `geom_smooth()` using formula &#39;y ~ x&#39; "],["generalized-additive-models.html", "4.7 Generalized Additive Models", " 4.7 Generalized Additive Models lm_gam &lt;- mgcv::gam(Sale_Price ~ s(total_sf) + bath + Lot_Area + Bedroom_AbvGr, data = dat) gam &lt;- ggeffects::ggpredict(lm_gam, &quot;total_sf&quot;) plot(gam, residuals = TRUE, residuals.line = TRUE) ## `geom_smooth()` using formula &#39;y ~ x&#39; "],["meeting-videos-3.html", "4.8 Meeting Videos", " 4.8 Meeting Videos 4.8.1 Cohort 1, Part 1 Meeting chat log CHAT LOG "],["classification.html", "Chapter 5 Classification", " Chapter 5 Classification Learning objectives: TBD "],["slide-2-1.html", "5.1 SLIDE 2", " 5.1 SLIDE 2 ADD SLIDE CONTENTS "],["slide-3-1.html", "5.2 SLIDE 3", " 5.2 SLIDE 3 MORE SLIDE CONTENTS "],["meeting-videos-4.html", "5.3 Meeting Videos", " 5.3 Meeting Videos 5.3.1 Cohort 1 Meeting chat log CHAT LOG "],["statistical-machine-learning.html", "Chapter 6 Statistical Machine Learning", " Chapter 6 Statistical Machine Learning Learning objectives: TBD "],["slide-2-2.html", "6.1 SLIDE 2", " 6.1 SLIDE 2 ADD SLIDE CONTENTS "],["slide-3-2.html", "6.2 SLIDE 3", " 6.2 SLIDE 3 MORE SLIDE CONTENTS "],["meeting-videos-5.html", "6.3 Meeting Videos", " 6.3 Meeting Videos 6.3.1 Cohort 1 Meeting chat log CHAT LOG "],["unsupervised-learning.html", "Chapter 7 Unsupervised Learning", " Chapter 7 Unsupervised Learning Learning objectives: TBD "],["slide-2-3.html", "7.1 SLIDE 2", " 7.1 SLIDE 2 ADD SLIDE CONTENTS "],["slide-3-3.html", "7.2 SLIDE 3", " 7.2 SLIDE 3 MORE SLIDE CONTENTS "],["meeting-videos-6.html", "7.3 Meeting Videos", " 7.3 Meeting Videos 7.3.1 Cohort 1 Meeting chat log CHAT LOG "]]
